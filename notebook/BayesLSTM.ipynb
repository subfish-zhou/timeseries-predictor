{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步，导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from blitz.modules import BayesianLSTM\n",
    "from blitz.utils import variational_estimator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "\n",
    "我们的数据集将由标准化股票价格的时间戳组成，并且具有一个形如(banch_size,sequence_length,observation_length)的shape。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      time    consume\n",
      "0  2010/01  246.67281\n",
      "1  2010/02  253.88689\n",
      "2  2010/03  248.72287\n",
      "3  2010/04  262.38641\n",
      "4  2010/05  243.05669\n",
      "(144, 2)\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "#导入数据\n",
    "df=pd.read_excel('../data/equipment_loss_rate.xlsx',sheet_name='warship_generate')\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(type(df['consume'].values[0]))#看看数据类型是不是float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变量解释：\n",
    "\n",
    "df:保存表格数据的DataFrame\n",
    "\n",
    "consume:未标准化的series\n",
    "\n",
    "consume_arr:未标准化的 n*1 arr\n",
    "\n",
    "consume_scale:标准化的 n*1 arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "\n",
    "consume=df['consume']\n",
    "\n",
    "scaler=StandardScaler()\n",
    "consume_arr=np.array(consume).reshape(-1,1)\n",
    "consume_scale=scaler.fit_transform(consume_arr) #标准化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个函数来按照时间戳转换我们的股价历史记录。为此，使用最大长度等于我们正在使用的时间戳大小的双端队列，我们将每个数据点添加到双端队列，然后将其副本附加到主时间戳列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_size=12\n",
    "\n",
    "def create_timestamps_ds(series,timestep_size=windows_size):\n",
    "    time_stamps=[]\n",
    "    labels=[]\n",
    "    aux_deque=deque(maxlen=timestep_size)\n",
    "\n",
    "    #starting the timestep deque\n",
    "    for i in range(timestep_size):\n",
    "        aux_deque.append(0)\n",
    "\n",
    "    #feed the timestamps list\n",
    "    for i in range(len(series)-1):\n",
    "        aux_deque.append(series[i])\n",
    "        time_stamps.append(list(aux_deque))\n",
    "\n",
    "    #feed the labels list\n",
    "    for i in range(len(series)-1):\n",
    "        labels.append(series[i+1])\n",
    "\n",
    "    assert len(time_stamps)==len(labels),'something went wrong'\n",
    "\n",
    "    #torch-tensoring it\n",
    "    features=torch.tensor(time_stamps[timestep_size:]).float()\n",
    "    labels=torch.tensor(labels[timestep_size:]).float()\n",
    "\n",
    "    return features,labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的网络接受variantal_estimator装饰器，该装饰器可简化对贝叶斯神经网络损失的采样。我们的网络具有一个贝叶斯LSTM层，参数设置为in_features=1以及out_features=10，后跟一个nn.Linear(10,1),该层输出股票的标准化价格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#神经网络类\n",
    "@variational_estimator\n",
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN,self).__init__()\n",
    "        self.lstm_1=BayesianLSTM(1,100)\n",
    "        self.linear=nn.Linear(100,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x_,_=self.lstm_1(x)\n",
    "\n",
    "        #gathering only the latent end-of-sequence for the linear layer\n",
    "        x_=x_[:,-1,:]\n",
    "        x_=self.linear(x_)\n",
    "        return x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如您所见，该网络可以正常工作，唯一的不同点是BayesianLSTM层和variantal_estimator装饰器，但其行为与一般的Torch对象相同。\n",
    "\n",
    "完成后，我们可以创建我们的神经网络对象，分割数据集并进入训练循环：\n",
    "\n",
    "## 创建对象\n",
    "\n",
    "我们现在可以创建损失函数、神经网络、优化器和dataloader。请注意，我们不是随机分割数据集，因为我们将使用最后一批时间戳来计算模型。由于我们的数据集很小，我们不会对训练集创建dataloader。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-95e61b3afc7c>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train,X_test,y_train,y_test=torch.tensor(X_train).to(device),\\\n",
      "<ipython-input-29-95e61b3afc7c>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(X_test).to(device),\\\n",
      "<ipython-input-29-95e61b3afc7c>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_train).to(device),\\\n",
      "<ipython-input-29-95e61b3afc7c>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y_test).to(device)\n"
     ]
    }
   ],
   "source": [
    "#将模型放到cuda上\n",
    "print(torch.cuda.is_available())\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "Xs,ys=create_timestamps_ds(consume_scale)\n",
    "X_train,X_test,y_train,y_test=train_test_split(Xs,ys,test_size=.15,random_state=42,shuffle=False)\n",
    "X_train,X_test,y_train,y_test=torch.tensor(X_train).to(device),\\\n",
    "                              torch.tensor(X_test).to(device),\\\n",
    "                              torch.tensor(y_train).to(device),\\\n",
    "                              torch.tensor(y_test).to(device)\n",
    "\n",
    "ds=torch.utils.data.TensorDataset(X_train,y_train)\n",
    "dataloader_train=torch.utils.data.DataLoader(ds,batch_size=72,shuffle=True)\n",
    "\n",
    "net=NN().to(device)\n",
    "\n",
    "#我们将使用MSE损失函数和学习率维0.001的Adam优化器\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.Adam(net.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练循环\n",
    "\n",
    "对于训练循环，我们将使用添加了variational_estimator的sample_wlbo方法。他对X个样本的损失进行平均，并帮助我们轻松地用蒙特卡洛估计来计算损失。\n",
    "\n",
    "为了使网络正常工作，网络forward方法的输出必须与传入损失函数对象的标签形状一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 20 Val-loss: 0.9185\n",
      "Iteration: 40 Val-loss: 0.8508\n",
      "Iteration: 60 Val-loss: 0.8321\n",
      "Iteration: 80 Val-loss: 0.8347\n",
      "Iteration: 100 Val-loss: 0.8456\n",
      "Iteration: 120 Val-loss: 0.8775\n",
      "Iteration: 140 Val-loss: 0.8962\n",
      "Iteration: 160 Val-loss: 0.9243\n",
      "Iteration: 180 Val-loss: 0.9396\n",
      "Iteration: 200 Val-loss: 0.9518\n",
      "Iteration: 220 Val-loss: 0.9800\n",
      "Iteration: 240 Val-loss: 0.9928\n",
      "Iteration: 260 Val-loss: 1.0089\n",
      "Iteration: 280 Val-loss: 1.0199\n",
      "Iteration: 300 Val-loss: 1.0277\n",
      "Iteration: 320 Val-loss: 1.0406\n",
      "Iteration: 340 Val-loss: 1.0478\n",
      "Iteration: 360 Val-loss: 1.0551\n",
      "Iteration: 380 Val-loss: 1.0617\n",
      "Iteration: 400 Val-loss: 1.0667\n",
      "Iteration: 420 Val-loss: 1.0723\n",
      "Iteration: 440 Val-loss: 1.0743\n",
      "Iteration: 460 Val-loss: 1.0637\n",
      "Iteration: 480 Val-loss: 1.0793\n",
      "Iteration: 500 Val-loss: 1.0774\n",
      "Iteration: 520 Val-loss: 1.0783\n",
      "Iteration: 540 Val-loss: 1.0903\n",
      "Iteration: 560 Val-loss: 1.0837\n",
      "Iteration: 580 Val-loss: 1.0940\n",
      "Iteration: 600 Val-loss: 1.0943\n",
      "Iteration: 620 Val-loss: 1.0987\n",
      "Iteration: 640 Val-loss: 1.0888\n",
      "Iteration: 660 Val-loss: 1.0825\n",
      "Iteration: 680 Val-loss: 1.0820\n",
      "Iteration: 700 Val-loss: 1.0772\n",
      "Iteration: 720 Val-loss: 1.0827\n",
      "Iteration: 740 Val-loss: 1.0834\n",
      "Iteration: 760 Val-loss: 1.0680\n",
      "Iteration: 780 Val-loss: 1.0967\n",
      "Iteration: 800 Val-loss: 1.0896\n",
      "Iteration: 820 Val-loss: 1.0844\n",
      "Iteration: 840 Val-loss: 1.0978\n",
      "Iteration: 860 Val-loss: 1.0847\n",
      "Iteration: 880 Val-loss: 1.0888\n",
      "Iteration: 900 Val-loss: 1.0890\n",
      "Iteration: 920 Val-loss: 1.0638\n",
      "Iteration: 940 Val-loss: 1.0690\n",
      "Iteration: 960 Val-loss: 1.0751\n",
      "Iteration: 980 Val-loss: 1.0734\n",
      "Iteration: 1000 Val-loss: 1.0547\n",
      "Iteration: 1020 Val-loss: 1.0821\n",
      "Iteration: 1040 Val-loss: 1.0857\n",
      "Iteration: 1060 Val-loss: 1.0456\n",
      "Iteration: 1080 Val-loss: 1.0902\n",
      "Iteration: 1100 Val-loss: 1.0632\n",
      "Iteration: 1120 Val-loss: 1.0792\n",
      "Iteration: 1140 Val-loss: 1.0600\n",
      "Iteration: 1160 Val-loss: 1.0574\n",
      "Iteration: 1180 Val-loss: 1.0515\n",
      "Iteration: 1200 Val-loss: 1.0440\n",
      "Iteration: 1220 Val-loss: 1.0404\n",
      "Iteration: 1240 Val-loss: 1.0652\n",
      "Iteration: 1260 Val-loss: 1.0584\n",
      "Iteration: 1280 Val-loss: 1.0506\n",
      "Iteration: 1300 Val-loss: 1.0040\n",
      "Iteration: 1320 Val-loss: 1.0287\n",
      "Iteration: 1340 Val-loss: 1.0225\n",
      "Iteration: 1360 Val-loss: 0.9848\n",
      "Iteration: 1380 Val-loss: 1.0102\n",
      "Iteration: 1400 Val-loss: 0.9762\n",
      "Iteration: 1420 Val-loss: 0.9919\n",
      "Iteration: 1440 Val-loss: 1.0054\n",
      "Iteration: 1460 Val-loss: 1.0224\n",
      "Iteration: 1480 Val-loss: 0.9781\n",
      "Iteration: 1500 Val-loss: 1.0234\n",
      "Iteration: 1520 Val-loss: 0.9803\n",
      "Iteration: 1540 Val-loss: 0.9636\n",
      "Iteration: 1560 Val-loss: 0.9673\n",
      "Iteration: 1580 Val-loss: 0.9500\n",
      "Iteration: 1600 Val-loss: 0.9334\n",
      "Iteration: 1620 Val-loss: 0.9020\n",
      "Iteration: 1640 Val-loss: 0.9721\n",
      "Iteration: 1660 Val-loss: 0.9542\n",
      "Iteration: 1680 Val-loss: 0.9393\n",
      "Iteration: 1700 Val-loss: 0.9299\n",
      "Iteration: 1720 Val-loss: 0.9749\n",
      "Iteration: 1740 Val-loss: 0.9642\n",
      "Iteration: 1760 Val-loss: 0.9106\n",
      "Iteration: 1780 Val-loss: 0.8927\n",
      "Iteration: 1800 Val-loss: 0.9217\n",
      "Iteration: 1820 Val-loss: 0.9211\n",
      "Iteration: 1840 Val-loss: 0.9027\n",
      "Iteration: 1860 Val-loss: 0.9152\n",
      "Iteration: 1880 Val-loss: 0.8853\n",
      "Iteration: 1900 Val-loss: 0.8409\n",
      "Iteration: 1920 Val-loss: 0.8638\n",
      "Iteration: 1940 Val-loss: 0.7900\n",
      "Iteration: 1960 Val-loss: 0.7957\n",
      "Iteration: 1980 Val-loss: 0.8336\n",
      "Iteration: 2000 Val-loss: 0.8383\n",
      "Iteration: 2020 Val-loss: 0.9219\n",
      "Iteration: 2040 Val-loss: 0.8048\n",
      "Iteration: 2060 Val-loss: 0.8339\n",
      "Iteration: 2080 Val-loss: 0.9089\n",
      "Iteration: 2100 Val-loss: 0.8342\n",
      "Iteration: 2120 Val-loss: 0.8266\n",
      "Iteration: 2140 Val-loss: 0.7985\n",
      "Iteration: 2160 Val-loss: 0.7045\n",
      "Iteration: 2180 Val-loss: 0.7394\n",
      "Iteration: 2200 Val-loss: 0.8334\n",
      "Iteration: 2220 Val-loss: 0.7134\n",
      "Iteration: 2240 Val-loss: 0.7773\n",
      "Iteration: 2260 Val-loss: 0.7665\n",
      "Iteration: 2280 Val-loss: 0.7754\n",
      "Iteration: 2300 Val-loss: 0.7480\n",
      "Iteration: 2320 Val-loss: 0.7290\n",
      "Iteration: 2340 Val-loss: 0.7599\n",
      "Iteration: 2360 Val-loss: 0.7445\n",
      "Iteration: 2380 Val-loss: 0.7317\n",
      "Iteration: 2400 Val-loss: 0.6920\n",
      "Iteration: 2420 Val-loss: 0.6926\n",
      "Iteration: 2440 Val-loss: 0.6802\n",
      "Iteration: 2460 Val-loss: 0.6175\n",
      "Iteration: 2480 Val-loss: 0.7070\n",
      "Iteration: 2500 Val-loss: 0.6869\n",
      "Iteration: 2520 Val-loss: 0.6583\n",
      "Iteration: 2540 Val-loss: 0.6938\n",
      "Iteration: 2560 Val-loss: 0.6927\n",
      "Iteration: 2580 Val-loss: 0.7608\n",
      "Iteration: 2600 Val-loss: 0.5975\n",
      "Iteration: 2620 Val-loss: 0.5795\n",
      "Iteration: 2640 Val-loss: 0.6276\n",
      "Iteration: 2660 Val-loss: 0.6312\n",
      "Iteration: 2680 Val-loss: 0.6367\n",
      "Iteration: 2700 Val-loss: 0.6735\n",
      "Iteration: 2720 Val-loss: 0.7220\n",
      "Iteration: 2740 Val-loss: 0.5455\n",
      "Iteration: 2760 Val-loss: 0.6983\n",
      "Iteration: 2780 Val-loss: 0.6386\n",
      "Iteration: 2800 Val-loss: 0.7323\n",
      "Iteration: 2820 Val-loss: 0.5684\n",
      "Iteration: 2840 Val-loss: 0.5289\n",
      "Iteration: 2860 Val-loss: 0.5549\n",
      "Iteration: 2880 Val-loss: 0.5853\n",
      "Iteration: 2900 Val-loss: 0.6045\n",
      "Iteration: 2920 Val-loss: 0.5709\n",
      "Iteration: 2940 Val-loss: 0.5561\n",
      "Iteration: 2960 Val-loss: 0.4628\n",
      "Iteration: 2980 Val-loss: 0.5895\n",
      "Iteration: 3000 Val-loss: 0.5827\n",
      "Iteration: 3020 Val-loss: 0.6160\n",
      "Iteration: 3040 Val-loss: 0.5652\n",
      "Iteration: 3060 Val-loss: 0.5924\n",
      "Iteration: 3080 Val-loss: 0.5315\n",
      "Iteration: 3100 Val-loss: 0.5605\n",
      "Iteration: 3120 Val-loss: 0.5632\n",
      "Iteration: 3140 Val-loss: 0.5826\n",
      "Iteration: 3160 Val-loss: 0.5377\n",
      "Iteration: 3180 Val-loss: 0.4631\n",
      "Iteration: 3200 Val-loss: 0.6010\n",
      "Iteration: 3220 Val-loss: 0.4170\n",
      "Iteration: 3240 Val-loss: 0.5308\n",
      "Iteration: 3260 Val-loss: 0.6990\n",
      "Iteration: 3280 Val-loss: 0.3779\n",
      "Iteration: 3300 Val-loss: 0.5219\n",
      "Iteration: 3320 Val-loss: 0.4290\n",
      "Iteration: 3340 Val-loss: 0.5019\n",
      "Iteration: 3360 Val-loss: 0.4671\n",
      "Iteration: 3380 Val-loss: 0.5286\n",
      "Iteration: 3400 Val-loss: 0.4634\n",
      "Iteration: 3420 Val-loss: 0.5929\n",
      "Iteration: 3440 Val-loss: 0.3918\n",
      "Iteration: 3460 Val-loss: 0.3821\n",
      "Iteration: 3480 Val-loss: 0.4200\n",
      "Iteration: 3500 Val-loss: 0.4557\n",
      "Iteration: 3520 Val-loss: 0.4463\n",
      "Iteration: 3540 Val-loss: 0.4141\n",
      "Iteration: 3560 Val-loss: 0.6031\n",
      "Iteration: 3580 Val-loss: 0.5413\n",
      "Iteration: 3600 Val-loss: 0.3368\n",
      "Iteration: 3620 Val-loss: 0.3440\n",
      "Iteration: 3640 Val-loss: 0.3916\n",
      "Iteration: 3660 Val-loss: 0.4525\n",
      "Iteration: 3680 Val-loss: 0.4615\n",
      "Iteration: 3700 Val-loss: 0.4555\n",
      "Iteration: 3720 Val-loss: 0.6077\n",
      "Iteration: 3740 Val-loss: 0.4250\n",
      "Iteration: 3760 Val-loss: 0.4956\n",
      "Iteration: 3780 Val-loss: 0.3658\n",
      "Iteration: 3800 Val-loss: 0.3901\n",
      "Iteration: 3820 Val-loss: 0.3876\n",
      "Iteration: 3840 Val-loss: 0.4831\n",
      "Iteration: 3860 Val-loss: 0.3581\n",
      "Iteration: 3880 Val-loss: 0.7238\n",
      "Iteration: 3900 Val-loss: 0.3838\n",
      "Iteration: 3920 Val-loss: 0.5445\n",
      "Iteration: 3940 Val-loss: 0.3045\n",
      "Iteration: 3960 Val-loss: 0.2953\n",
      "Iteration: 3980 Val-loss: 0.2930\n",
      "Iteration: 4000 Val-loss: 0.3306\n",
      "Iteration: 4020 Val-loss: 0.3128\n",
      "Iteration: 4040 Val-loss: 0.3863\n",
      "Iteration: 4060 Val-loss: 0.3205\n",
      "Iteration: 4080 Val-loss: 0.6072\n",
      "Iteration: 4100 Val-loss: 0.3077\n",
      "Iteration: 4120 Val-loss: 0.3282\n",
      "Iteration: 4140 Val-loss: 0.2744\n",
      "Iteration: 4160 Val-loss: 0.8114\n",
      "Iteration: 4180 Val-loss: 0.4303\n",
      "Iteration: 4200 Val-loss: 0.5929\n",
      "Iteration: 4220 Val-loss: 0.2824\n",
      "Iteration: 4240 Val-loss: 0.6460\n",
      "Iteration: 4260 Val-loss: 0.5387\n",
      "Iteration: 4280 Val-loss: 0.2432\n",
      "Iteration: 4300 Val-loss: 0.4383\n",
      "Iteration: 4320 Val-loss: 0.2582\n",
      "Iteration: 4340 Val-loss: 0.3009\n",
      "Iteration: 4360 Val-loss: 0.3161\n",
      "Iteration: 4380 Val-loss: 0.2789\n",
      "Iteration: 4400 Val-loss: 0.4260\n",
      "Iteration: 4420 Val-loss: 0.4941\n",
      "Iteration: 4440 Val-loss: 0.3753\n",
      "Iteration: 4460 Val-loss: 0.4540\n",
      "Iteration: 4480 Val-loss: 0.2586\n",
      "Iteration: 4500 Val-loss: 0.3165\n",
      "Iteration: 4520 Val-loss: 0.3053\n",
      "Iteration: 4540 Val-loss: 0.2532\n",
      "Iteration: 4560 Val-loss: 0.3408\n",
      "Iteration: 4580 Val-loss: 0.4435\n",
      "Iteration: 4600 Val-loss: 0.6612\n",
      "Iteration: 4620 Val-loss: 0.5972\n",
      "Iteration: 4640 Val-loss: 0.4815\n",
      "Iteration: 4660 Val-loss: 0.2961\n",
      "Iteration: 4680 Val-loss: 0.4252\n",
      "Iteration: 4700 Val-loss: 0.3913\n",
      "Iteration: 4720 Val-loss: 0.2789\n",
      "Iteration: 4740 Val-loss: 0.3104\n",
      "Iteration: 4760 Val-loss: 0.5079\n",
      "Iteration: 4780 Val-loss: 0.5000\n",
      "Iteration: 4800 Val-loss: 0.4413\n",
      "Iteration: 4820 Val-loss: 0.5193\n",
      "Iteration: 4840 Val-loss: 0.3690\n",
      "Iteration: 4860 Val-loss: 0.2347\n",
      "Iteration: 4880 Val-loss: 0.4374\n",
      "Iteration: 4900 Val-loss: 0.2980\n",
      "Iteration: 4920 Val-loss: 0.2537\n",
      "Iteration: 4940 Val-loss: 0.5185\n",
      "Iteration: 4960 Val-loss: 0.2629\n",
      "Iteration: 4980 Val-loss: 0.9335\n",
      "Iteration: 5000 Val-loss: 0.2992\n",
      "Iteration: 5020 Val-loss: 0.2383\n",
      "Iteration: 5040 Val-loss: 0.3072\n",
      "Iteration: 5060 Val-loss: 0.2641\n",
      "Iteration: 5080 Val-loss: 0.4100\n",
      "Iteration: 5100 Val-loss: 0.3053\n",
      "Iteration: 5120 Val-loss: 0.2477\n",
      "Iteration: 5140 Val-loss: 0.4753\n",
      "Iteration: 5160 Val-loss: 0.2740\n",
      "Iteration: 5180 Val-loss: 0.5713\n",
      "Iteration: 5200 Val-loss: 0.3825\n",
      "Iteration: 5220 Val-loss: 0.3205\n",
      "Iteration: 5240 Val-loss: 0.2918\n",
      "Iteration: 5260 Val-loss: 0.2494\n",
      "Iteration: 5280 Val-loss: 0.2676\n",
      "Iteration: 5300 Val-loss: 0.3753\n",
      "Iteration: 5320 Val-loss: 0.4132\n",
      "Iteration: 5340 Val-loss: 0.5138\n",
      "Iteration: 5360 Val-loss: 0.2929\n",
      "Iteration: 5380 Val-loss: 0.3821\n",
      "Iteration: 5400 Val-loss: 0.4573\n",
      "Iteration: 5420 Val-loss: 0.3202\n",
      "Iteration: 5440 Val-loss: 0.2645\n",
      "Iteration: 5460 Val-loss: 0.4487\n",
      "Iteration: 5480 Val-loss: 0.5333\n",
      "Iteration: 5500 Val-loss: 0.7518\n",
      "Iteration: 5520 Val-loss: 0.2767\n",
      "Iteration: 5540 Val-loss: 0.2371\n",
      "Iteration: 5560 Val-loss: 0.2500\n",
      "Iteration: 5580 Val-loss: 0.3451\n",
      "Iteration: 5600 Val-loss: 0.3778\n",
      "Iteration: 5620 Val-loss: 0.4473\n",
      "Iteration: 5640 Val-loss: 0.9327\n",
      "Iteration: 5660 Val-loss: 0.3047\n",
      "Iteration: 5680 Val-loss: 0.9322\n",
      "Iteration: 5700 Val-loss: 0.2599\n",
      "Iteration: 5720 Val-loss: 0.2565\n",
      "Iteration: 5740 Val-loss: 0.2563\n",
      "Iteration: 5760 Val-loss: 0.3896\n",
      "Iteration: 5780 Val-loss: 0.2873\n",
      "Iteration: 5800 Val-loss: 0.2947\n",
      "Iteration: 5820 Val-loss: 2.4180\n",
      "Iteration: 5840 Val-loss: 0.7813\n",
      "Iteration: 5860 Val-loss: 0.2640\n",
      "Iteration: 5880 Val-loss: 0.4677\n",
      "Iteration: 5900 Val-loss: 0.5784\n",
      "Iteration: 5920 Val-loss: 0.5026\n",
      "Iteration: 5940 Val-loss: 0.2746\n",
      "Iteration: 5960 Val-loss: 0.2405\n",
      "Iteration: 5980 Val-loss: 0.8242\n",
      "Iteration: 6000 Val-loss: 0.7081\n",
      "Iteration: 6020 Val-loss: 1.3891\n",
      "Iteration: 6040 Val-loss: 0.4525\n",
      "Iteration: 6060 Val-loss: 0.5611\n",
      "Iteration: 6080 Val-loss: 0.2331\n",
      "Iteration: 6100 Val-loss: 0.2517\n",
      "Iteration: 6120 Val-loss: 0.2422\n",
      "Iteration: 6140 Val-loss: 0.4454\n",
      "Iteration: 6160 Val-loss: 0.2695\n",
      "Iteration: 6180 Val-loss: 0.2566\n",
      "Iteration: 6200 Val-loss: 0.2829\n",
      "Iteration: 6220 Val-loss: 0.5671\n",
      "Iteration: 6240 Val-loss: 0.2366\n",
      "Iteration: 6260 Val-loss: 0.2487\n",
      "Iteration: 6280 Val-loss: 0.2589\n",
      "Iteration: 6300 Val-loss: 0.8022\n",
      "Iteration: 6320 Val-loss: 0.8929\n",
      "Iteration: 6340 Val-loss: 0.2777\n",
      "Iteration: 6360 Val-loss: 0.2572\n",
      "Iteration: 6380 Val-loss: 0.2624\n",
      "Iteration: 6400 Val-loss: 0.3356\n",
      "Iteration: 6420 Val-loss: 0.2523\n",
      "Iteration: 6440 Val-loss: 0.3001\n",
      "Iteration: 6460 Val-loss: 0.8950\n",
      "Iteration: 6480 Val-loss: 0.2600\n",
      "Iteration: 6500 Val-loss: 0.2543\n",
      "Iteration: 6520 Val-loss: 0.8256\n",
      "Iteration: 6540 Val-loss: 0.2546\n",
      "Iteration: 6560 Val-loss: 0.4055\n",
      "Iteration: 6580 Val-loss: 1.1432\n",
      "Iteration: 6600 Val-loss: 0.2434\n",
      "Iteration: 6620 Val-loss: 0.5227\n",
      "Iteration: 6640 Val-loss: 0.4914\n",
      "Iteration: 6660 Val-loss: 0.5650\n",
      "Iteration: 6680 Val-loss: 0.2893\n",
      "Iteration: 6700 Val-loss: 0.5760\n",
      "Iteration: 6720 Val-loss: 0.2500\n",
      "Iteration: 6740 Val-loss: 0.4300\n",
      "Iteration: 6760 Val-loss: 1.5167\n",
      "Iteration: 6780 Val-loss: 0.3501\n",
      "Iteration: 6800 Val-loss: 0.4229\n",
      "Iteration: 6820 Val-loss: 0.4320\n",
      "Iteration: 6840 Val-loss: 0.7045\n",
      "Iteration: 6860 Val-loss: 0.6335\n",
      "Iteration: 6880 Val-loss: 0.3035\n",
      "Iteration: 6900 Val-loss: 0.3287\n",
      "Iteration: 6920 Val-loss: 0.9658\n",
      "Iteration: 6940 Val-loss: 0.3648\n",
      "Iteration: 6960 Val-loss: 0.4141\n",
      "Iteration: 6980 Val-loss: 0.8078\n",
      "Iteration: 7000 Val-loss: 0.4842\n",
      "Iteration: 7020 Val-loss: 0.2851\n",
      "Iteration: 7040 Val-loss: 0.3376\n",
      "Iteration: 7060 Val-loss: 0.3575\n",
      "Iteration: 7080 Val-loss: 0.4598\n",
      "Iteration: 7100 Val-loss: 1.1147\n",
      "Iteration: 7120 Val-loss: 0.6672\n",
      "Iteration: 7140 Val-loss: 0.5342\n",
      "Iteration: 7160 Val-loss: 0.8940\n",
      "Iteration: 7180 Val-loss: 0.2723\n",
      "Iteration: 7200 Val-loss: 0.3598\n",
      "Iteration: 7220 Val-loss: 0.3377\n",
      "Iteration: 7240 Val-loss: 0.8322\n",
      "Iteration: 7260 Val-loss: 0.2718\n",
      "Iteration: 7280 Val-loss: 0.9346\n",
      "Iteration: 7300 Val-loss: 0.4817\n",
      "Iteration: 7320 Val-loss: 0.7176\n",
      "Iteration: 7340 Val-loss: 0.2967\n",
      "Iteration: 7360 Val-loss: 0.9040\n",
      "Iteration: 7380 Val-loss: 0.3472\n",
      "Iteration: 7400 Val-loss: 0.8035\n",
      "Iteration: 7420 Val-loss: 0.3654\n",
      "Iteration: 7440 Val-loss: 0.2561\n",
      "Iteration: 7460 Val-loss: 1.0771\n",
      "Iteration: 7480 Val-loss: 0.2900\n",
      "Iteration: 7500 Val-loss: 0.6690\n",
      "Iteration: 7520 Val-loss: 0.8165\n",
      "Iteration: 7540 Val-loss: 0.3206\n",
      "Iteration: 7560 Val-loss: 1.4298\n",
      "Iteration: 7580 Val-loss: 0.4486\n",
      "Iteration: 7600 Val-loss: 0.3402\n",
      "Iteration: 7620 Val-loss: 0.2583\n",
      "Iteration: 7640 Val-loss: 1.2473\n",
      "Iteration: 7660 Val-loss: 0.6729\n",
      "Iteration: 7680 Val-loss: 1.4094\n",
      "Iteration: 7700 Val-loss: 2.4545\n",
      "Iteration: 7720 Val-loss: 0.6370\n",
      "Iteration: 7740 Val-loss: 1.7926\n",
      "Iteration: 7760 Val-loss: 1.6364\n",
      "Iteration: 7780 Val-loss: 0.2777\n",
      "Iteration: 7800 Val-loss: 0.4252\n",
      "Iteration: 7820 Val-loss: 0.4818\n",
      "Iteration: 7840 Val-loss: 2.0825\n",
      "Iteration: 7860 Val-loss: 3.6226\n",
      "Iteration: 7880 Val-loss: 0.5530\n",
      "Iteration: 7900 Val-loss: 0.9804\n",
      "Iteration: 7920 Val-loss: 0.8698\n",
      "Iteration: 7940 Val-loss: 0.8840\n",
      "Iteration: 7960 Val-loss: 0.4416\n",
      "Iteration: 7980 Val-loss: 0.5319\n",
      "Iteration: 8000 Val-loss: 0.3138\n",
      "Iteration: 8020 Val-loss: 1.6478\n",
      "Iteration: 8040 Val-loss: 1.5167\n",
      "Iteration: 8060 Val-loss: 0.9912\n",
      "Iteration: 8080 Val-loss: 0.2766\n",
      "Iteration: 8100 Val-loss: 1.3237\n",
      "Iteration: 8120 Val-loss: 0.2233\n",
      "Iteration: 8140 Val-loss: 0.2551\n",
      "Iteration: 8160 Val-loss: 1.4738\n",
      "Iteration: 8180 Val-loss: 0.8920\n",
      "Iteration: 8200 Val-loss: 0.4092\n",
      "Iteration: 8220 Val-loss: 0.3088\n",
      "Iteration: 8240 Val-loss: 1.4579\n",
      "Iteration: 8260 Val-loss: 0.9611\n",
      "Iteration: 8280 Val-loss: 1.0747\n",
      "Iteration: 8300 Val-loss: 0.2562\n",
      "Iteration: 8320 Val-loss: 0.4107\n",
      "Iteration: 8340 Val-loss: 0.6981\n",
      "Iteration: 8360 Val-loss: 0.2673\n",
      "Iteration: 8380 Val-loss: 0.3221\n",
      "Iteration: 8400 Val-loss: 0.6194\n",
      "Iteration: 8420 Val-loss: 1.2722\n",
      "Iteration: 8440 Val-loss: 0.5039\n",
      "Iteration: 8460 Val-loss: 0.8958\n",
      "Iteration: 8480 Val-loss: 0.4396\n",
      "Iteration: 8500 Val-loss: 0.3065\n",
      "Iteration: 8520 Val-loss: 0.3315\n",
      "Iteration: 8540 Val-loss: 0.2832\n",
      "Iteration: 8560 Val-loss: 0.3716\n",
      "Iteration: 8580 Val-loss: 0.5791\n",
      "Iteration: 8600 Val-loss: 0.3184\n",
      "Iteration: 8620 Val-loss: 1.2832\n",
      "Iteration: 8640 Val-loss: 0.4726\n",
      "Iteration: 8660 Val-loss: 2.2231\n",
      "Iteration: 8680 Val-loss: 0.2668\n",
      "Iteration: 8700 Val-loss: 1.1837\n",
      "Iteration: 8720 Val-loss: 1.5617\n",
      "Iteration: 8740 Val-loss: 0.3214\n",
      "Iteration: 8760 Val-loss: 7.3352\n",
      "Iteration: 8780 Val-loss: 0.6363\n",
      "Iteration: 8800 Val-loss: 0.4751\n",
      "Iteration: 8820 Val-loss: 0.9618\n",
      "Iteration: 8840 Val-loss: 0.6504\n",
      "Iteration: 8860 Val-loss: 0.3956\n",
      "Iteration: 8880 Val-loss: 0.4128\n",
      "Iteration: 8900 Val-loss: 1.3032\n",
      "Iteration: 8920 Val-loss: 0.3656\n",
      "Iteration: 8940 Val-loss: 0.2684\n",
      "Iteration: 8960 Val-loss: 0.1965\n",
      "Iteration: 8980 Val-loss: 2.6866\n",
      "Iteration: 9000 Val-loss: 0.9707\n",
      "Iteration: 9020 Val-loss: 1.9745\n",
      "Iteration: 9040 Val-loss: 0.7862\n",
      "Iteration: 9060 Val-loss: 3.4203\n",
      "Iteration: 9080 Val-loss: 1.3736\n",
      "Iteration: 9100 Val-loss: 1.4797\n",
      "Iteration: 9120 Val-loss: 0.3450\n",
      "Iteration: 9140 Val-loss: 1.7566\n",
      "Iteration: 9160 Val-loss: 0.2991\n",
      "Iteration: 9180 Val-loss: 1.5645\n",
      "Iteration: 9200 Val-loss: 0.5254\n",
      "Iteration: 9220 Val-loss: 0.7096\n",
      "Iteration: 9240 Val-loss: 0.4977\n",
      "Iteration: 9260 Val-loss: 4.2281\n",
      "Iteration: 9280 Val-loss: 0.2001\n",
      "Iteration: 9300 Val-loss: 1.9540\n",
      "Iteration: 9320 Val-loss: 0.2639\n",
      "Iteration: 9340 Val-loss: 0.6053\n",
      "Iteration: 9360 Val-loss: 0.2933\n",
      "Iteration: 9380 Val-loss: 0.5344\n",
      "Iteration: 9400 Val-loss: 0.3956\n",
      "Iteration: 9420 Val-loss: 0.6832\n",
      "Iteration: 9440 Val-loss: 0.3429\n",
      "Iteration: 9460 Val-loss: 0.3699\n",
      "Iteration: 9480 Val-loss: 2.4973\n",
      "Iteration: 9500 Val-loss: 0.2262\n",
      "Iteration: 9520 Val-loss: 0.3383\n",
      "Iteration: 9540 Val-loss: 0.2495\n",
      "Iteration: 9560 Val-loss: 1.7102\n",
      "Iteration: 9580 Val-loss: 1.6902\n",
      "Iteration: 9600 Val-loss: 0.8407\n",
      "Iteration: 9620 Val-loss: 0.4454\n",
      "Iteration: 9640 Val-loss: 0.5366\n",
      "Iteration: 9660 Val-loss: 0.2637\n",
      "Iteration: 9680 Val-loss: 1.1779\n",
      "Iteration: 9700 Val-loss: 3.9977\n",
      "Iteration: 9720 Val-loss: 1.7357\n",
      "Iteration: 9740 Val-loss: 0.6387\n",
      "Iteration: 9760 Val-loss: 0.6745\n",
      "Iteration: 9780 Val-loss: 0.8851\n",
      "Iteration: 9800 Val-loss: 0.6884\n",
      "Iteration: 9820 Val-loss: 1.2407\n",
      "Iteration: 9840 Val-loss: 0.2586\n",
      "Iteration: 9860 Val-loss: 0.9753\n",
      "Iteration: 9880 Val-loss: 2.8635\n",
      "Iteration: 9900 Val-loss: 0.3990\n",
      "Iteration: 9920 Val-loss: 0.6061\n",
      "Iteration: 9940 Val-loss: 0.2858\n",
      "Iteration: 9960 Val-loss: 1.8140\n",
      "Iteration: 9980 Val-loss: 1.2618\n",
      "Iteration: 10000 Val-loss: 0.3891\n",
      "Iteration: 10020 Val-loss: 0.3485\n",
      "Iteration: 10040 Val-loss: 1.6504\n",
      "Iteration: 10060 Val-loss: 3.5979\n",
      "Iteration: 10080 Val-loss: 0.2281\n",
      "Iteration: 10100 Val-loss: 2.9681\n",
      "Iteration: 10120 Val-loss: 0.2217\n",
      "Iteration: 10140 Val-loss: 1.7358\n",
      "Iteration: 10160 Val-loss: 0.4011\n",
      "Iteration: 10180 Val-loss: 2.0319\n",
      "Iteration: 10200 Val-loss: 0.8531\n",
      "Iteration: 10220 Val-loss: 1.0723\n",
      "Iteration: 10240 Val-loss: 0.3936\n",
      "Iteration: 10260 Val-loss: 0.7075\n",
      "Iteration: 10280 Val-loss: 0.3476\n",
      "Iteration: 10300 Val-loss: 0.6874\n",
      "Iteration: 10320 Val-loss: 1.6760\n",
      "Iteration: 10340 Val-loss: 2.7524\n",
      "Iteration: 10360 Val-loss: 1.1473\n",
      "Iteration: 10380 Val-loss: 0.2781\n",
      "Iteration: 10400 Val-loss: 0.4458\n",
      "Iteration: 10420 Val-loss: 0.9776\n",
      "Iteration: 10440 Val-loss: 0.2556\n",
      "Iteration: 10460 Val-loss: 0.8262\n",
      "Iteration: 10480 Val-loss: 0.3011\n",
      "Iteration: 10500 Val-loss: 1.2652\n",
      "Iteration: 10520 Val-loss: 1.7099\n",
      "Iteration: 10540 Val-loss: 0.2737\n",
      "Iteration: 10560 Val-loss: 0.2859\n",
      "Iteration: 10580 Val-loss: 0.3479\n",
      "Iteration: 10600 Val-loss: 0.2923\n",
      "Iteration: 10620 Val-loss: 1.6123\n",
      "Iteration: 10640 Val-loss: 1.9313\n",
      "Iteration: 10660 Val-loss: 1.8277\n",
      "Iteration: 10680 Val-loss: 0.2232\n",
      "Iteration: 10700 Val-loss: 0.3760\n",
      "Iteration: 10720 Val-loss: 4.8570\n",
      "Iteration: 10740 Val-loss: 0.4156\n",
      "Iteration: 10760 Val-loss: 1.1131\n",
      "Iteration: 10780 Val-loss: 4.5337\n",
      "Iteration: 10800 Val-loss: 0.3088\n",
      "Iteration: 10820 Val-loss: 0.3323\n",
      "Iteration: 10840 Val-loss: 0.2190\n",
      "Iteration: 10860 Val-loss: 1.2955\n",
      "Iteration: 10880 Val-loss: 0.2145\n",
      "Iteration: 10900 Val-loss: 6.7884\n",
      "Iteration: 10920 Val-loss: 0.3707\n",
      "Iteration: 10940 Val-loss: 0.3069\n",
      "Iteration: 10960 Val-loss: 0.6888\n",
      "Iteration: 10980 Val-loss: 2.1437\n",
      "Iteration: 11000 Val-loss: 0.2138\n",
      "Iteration: 11020 Val-loss: 2.0093\n",
      "Iteration: 11040 Val-loss: 0.2672\n",
      "Iteration: 11060 Val-loss: 0.2895\n",
      "Iteration: 11080 Val-loss: 0.3071\n",
      "Iteration: 11100 Val-loss: 1.3502\n",
      "Iteration: 11120 Val-loss: 0.9245\n",
      "Iteration: 11140 Val-loss: 0.5763\n",
      "Iteration: 11160 Val-loss: 0.7082\n",
      "Iteration: 11180 Val-loss: 2.3411\n",
      "Iteration: 11200 Val-loss: 0.2798\n",
      "Iteration: 11220 Val-loss: 1.5718\n",
      "Iteration: 11240 Val-loss: 1.3447\n",
      "Iteration: 11260 Val-loss: 0.3599\n",
      "Iteration: 11280 Val-loss: 0.3395\n",
      "Iteration: 11300 Val-loss: 4.2231\n",
      "Iteration: 11320 Val-loss: 1.7850\n",
      "Iteration: 11340 Val-loss: 2.6732\n",
      "Iteration: 11360 Val-loss: 0.4426\n",
      "Iteration: 11380 Val-loss: 0.2916\n",
      "Iteration: 11400 Val-loss: 0.7956\n",
      "Iteration: 11420 Val-loss: 3.1620\n",
      "Iteration: 11440 Val-loss: 0.3362\n",
      "Iteration: 11460 Val-loss: 0.2678\n",
      "Iteration: 11480 Val-loss: 0.5037\n",
      "Iteration: 11500 Val-loss: 1.1825\n",
      "Iteration: 11520 Val-loss: 0.4478\n",
      "Iteration: 11540 Val-loss: 0.3141\n",
      "Iteration: 11560 Val-loss: 0.5735\n",
      "Iteration: 11580 Val-loss: 0.6082\n",
      "Iteration: 11600 Val-loss: 2.0358\n",
      "Iteration: 11620 Val-loss: 0.2676\n",
      "Iteration: 11640 Val-loss: 0.5713\n",
      "Iteration: 11660 Val-loss: 1.3684\n",
      "Iteration: 11680 Val-loss: 2.0285\n",
      "Iteration: 11700 Val-loss: 2.2490\n",
      "Iteration: 11720 Val-loss: 0.6242\n",
      "Iteration: 11740 Val-loss: 1.3780\n",
      "Iteration: 11760 Val-loss: 4.7746\n",
      "Iteration: 11780 Val-loss: 0.3704\n",
      "Iteration: 11800 Val-loss: 1.0897\n",
      "Iteration: 11820 Val-loss: 1.0735\n",
      "Iteration: 11840 Val-loss: 2.7072\n",
      "Iteration: 11860 Val-loss: 0.3577\n",
      "Iteration: 11880 Val-loss: 1.0214\n",
      "Iteration: 11900 Val-loss: 1.1577\n",
      "Iteration: 11920 Val-loss: 0.6656\n",
      "Iteration: 11940 Val-loss: 3.1084\n",
      "Iteration: 11960 Val-loss: 0.3921\n",
      "Iteration: 11980 Val-loss: 1.2466\n",
      "Iteration: 12000 Val-loss: 1.1431\n"
     ]
    }
   ],
   "source": [
    "iteration=0\n",
    "for epoch in range(6000):\n",
    "    for i,(datapoints,labels) in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss=net.sample_elbo(inputs=datapoints,labels=labels,criterion=criterion,sample_nbr=3)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iteration+=1\n",
    "        if iteration%20==0:\n",
    "            preds_test=net(X_test)[:,0].unsqueeze(1)\n",
    "            loss_test=criterion(preds_test,y_test)\n",
    "            print(\"Iteration: {} Val-loss: {:.4f}\".format(str(iteration),loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估模型并计算置信区间\n",
    "\n",
    "我们将首先创建一个具有要绘制的真实数据的dataframe："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = df['consume'][1:][windows_size:]\n",
    "\n",
    "df_pred = pd.DataFrame(original)\n",
    "df_pred[\"Date\"] = df.time\n",
    "df[\"Date\"] = pd.to_datetime(df_pred[\"Date\"])\n",
    "df_pred = df_pred.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要绘制置信区间，我们必须创建一个函数来预测同一数据X次，然后收集其均值和标准差。同时，在查询真实数据之前，我们必须设置将尝试预测的窗口大小。\n",
    "\n",
    "让我们看一下预测函数的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_consume_future(X_test,future_length,sample_nbr=10):\n",
    "    global windows_size\n",
    "    global X_train\n",
    "    global Xs\n",
    "    global scaler\n",
    "\n",
    "    #creating auxiliar variables for future prediction\n",
    "    preds_test=[]\n",
    "    test_begin=X_test[0:1,:,:]\n",
    "    test_deque=deque(test_begin[0,:,0].tolist(),maxlen=windows_size)\n",
    "\n",
    "    idx_pred=np.arange(len(X_train),len(Xs))\n",
    "\n",
    "    #predict it and append to list\n",
    "    for i in range(len(X_test)):\n",
    "        #print(i)\n",
    "        as_net_input = torch.tensor(test_deque).unsqueeze(0).unsqueeze(2)\n",
    "        pred = [net(as_net_input).cpu().item() for i in range(sample_nbr)]\n",
    "        \n",
    "        \n",
    "        test_deque.append(torch.tensor(pred).mean().cpu().item())\n",
    "        preds_test.append(pred)\n",
    "        \n",
    "        if i % future_length == 0:\n",
    "            #our inptus become the i index of our X_test\n",
    "            #That tweak just helps us with shape issues\n",
    "            test_begin = X_test[i:i+1, :, :]\n",
    "            test_deque = deque(test_begin[0,:,0].tolist(), maxlen=windows_size)\n",
    "\n",
    "    #preds_test = np.array(preds_test).reshape(-1, 1)\n",
    "    #preds_test_unscaled = scaler.inverse_transform(preds_test)\n",
    "    \n",
    "    return idx_pred, preds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将置信区间保存下来，确定我们置信区间的宽度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_intervals(preds_test, ci_multiplier):\n",
    "    global scaler\n",
    "    \n",
    "    preds_test = torch.tensor(preds_test)\n",
    "    \n",
    "    pred_mean = preds_test.mean(1)\n",
    "    pred_std = preds_test.std(1).detach().cpu().numpy()\n",
    "\n",
    "    pred_std = torch.tensor((pred_std))\n",
    "    \n",
    "    upper_bound = pred_mean + (pred_std * ci_multiplier)\n",
    "    lower_bound = pred_mean - (pred_std * ci_multiplier)\n",
    "    #gather unscaled confidence intervals\n",
    "\n",
    "    pred_mean_final = pred_mean.unsqueeze(1).detach().cpu().numpy()\n",
    "    pred_mean_unscaled = scaler.inverse_transform(pred_mean_final)\n",
    "\n",
    "    upper_bound_unscaled = upper_bound.unsqueeze(1).detach().cpu().numpy()\n",
    "    upper_bound_unscaled = scaler.inverse_transform(upper_bound_unscaled)\n",
    "    \n",
    "    lower_bound_unscaled = lower_bound.unsqueeze(1).detach().cpu().numpy()\n",
    "    lower_bound_unscaled = scaler.inverse_transform(lower_bound_unscaled)\n",
    "    \n",
    "    return pred_mean_unscaled, upper_bound_unscaled, lower_bound_unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们使用的样本数量很少，因此用一个很高的标准差对其进行了补偿。我们的网络将尝试预测7月，然后将参考数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.to('cpu')\n",
    "net.to('cpu')\n",
    "\n",
    "future_length=7\n",
    "sample_nbr=4\n",
    "ci_multiplier=10\n",
    "idx_pred, preds_test = pre_consume_future(X_test, future_length, sample_nbr)\n",
    "pred_mean_unscaled, upper_bound_unscaled, lower_bound_unscaled = get_confidence_intervals(preds_test,\n",
    "                                                                                          ci_multiplier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过查看实际值是否低于上限并高于下限来检查置信区间。 设置好参数后，您应该拥有95％的置信区间，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 our predictions are in our confidence interval\n"
     ]
    }
   ],
   "source": [
    "y = np.array(df.consume[-20:]).reshape(-1, 1) #siifish:改变了-750\n",
    "under_upper = upper_bound_unscaled > y\n",
    "over_lower = lower_bound_unscaled < y\n",
    "total = (under_upper == over_lower)\n",
    "\n",
    "print(\"{} our predictions are in our confidence interval\".format(np.mean(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c989438d30>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {\"ytick.color\" : \"w\",\n",
    "          \"xtick.color\" : \"w\",\n",
    "          \"axes.labelcolor\" : \"w\",\n",
    "          \"axes.edgecolor\" : \"w\"}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "plt.title(\"IBM Stock prices\", color=\"white\")\n",
    "\n",
    "plt.plot(df_pred.index,\n",
    "         df_pred.consume,\n",
    "         color='black',\n",
    "         label=\"Real\")\n",
    "\n",
    "plt.plot(idx_pred,\n",
    "         pred_mean_unscaled,\n",
    "         label=\"Prediction for {} days, than consult\".format(future_length),\n",
    "         color=\"red\")\n",
    "\n",
    "plt.fill_between(x=idx_pred,\n",
    "                 y1=upper_bound_unscaled[:,0],\n",
    "                 y2=lower_bound_unscaled[:,0],\n",
    "                 facecolor='green',\n",
    "                 label=\"Confidence interval\",\n",
    "                 alpha=0.5)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f133408d770090f945e2095d28b8c545e34b9149b078719911919e45c5c0f91"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
